{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d13d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/usr/src/code/src')\n",
    "os.chdir('/usr/src/code/')\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.models.UNet import UNet, UNet2DWrapper, UNet_res, UNet_Tranformer\n",
    "from src.utils.other_utils import *\n",
    "from src.pdlmc.constraints import classifier_constraint, brightness_constraint, center_constraint\n",
    "from src.utils.samplers import *\n",
    "from src.utils.trainers import *\n",
    "from src.utils.schedulers import *\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff5fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28\n",
    "IMG_CH = 1\n",
    "BATCH_SIZE = 128\n",
    "N_CLASSES = 10\n",
    "timesteps = 250\n",
    "sigma = 25.0\n",
    "eps = 1e-3\n",
    "\n",
    "model = UNet_Tranformer(partial(marginal_prob_std, sigma=sigma, device=device))\n",
    "model.load_state_dict(torch.load(\"unet_transformer.pth\"))\n",
    "model = model.to(device)\n",
    "classifier = torch.load(\"mnist_classifier.pkl\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00075ba0",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf218257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t Sampling images: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timesteps = 400\n",
    "eps = 1e-3\n",
    "\n",
    "vesde_sampler = VESDESampler(device=device, img_ch=IMG_CH, img_size=IMG_SIZE)\n",
    "sample_timesteps = torch.linspace(eps, 1.0, timesteps, device=device)\n",
    "img = sample_images(\n",
    "    model=model,\n",
    "    sampler=vesde_sampler,\n",
    "    timesteps=sample_timesteps,\n",
    "    device=device,\n",
    "    sample_size=1,\n",
    "    plot=True,\n",
    "    save=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74329636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t Sampling images:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "gfuncs = [\n",
    "    partial(classifier_constraint, classifier=classifier, target_class=9, epsilon=0.01)\n",
    "]\n",
    "\n",
    "lmc_steps: int = 1\n",
    "update_steps: int = 3\n",
    "step_size: float = 2\n",
    "step_size_lambda: float = 0.2\n",
    "timesteps = 400\n",
    "\n",
    "pdlmc_sampler = PDLMCVESampler(\n",
    "    device=device,\n",
    "    img_ch=IMG_CH,\n",
    "    img_size=IMG_SIZE,\n",
    "    gfuncs=gfuncs,\n",
    "    lmc_steps=lmc_steps,\n",
    "    step_size=step_size,\n",
    "    step_size_lambda=step_size_lambda,\n",
    ")\n",
    "\n",
    "sample_timesteps = torch.linspace(eps, 1.0, timesteps, device=device)\n",
    "img = sample_images(\n",
    "    model=model,\n",
    "    sampler= pdlmc_sampler,\n",
    "    timesteps=sample_timesteps,\n",
    "    device=device,\n",
    "    sample_size=1,\n",
    "    plot=True,\n",
    "    save=False,\n",
    "    update_steps = update_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719bdbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t Sampling images:   1%|          | 1/100 [00:01<02:25,  1.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m vesde_sampler = VESDESampler(device=device, img_ch=IMG_CH, img_size=IMG_SIZE)\n\u001b[32m      6\u001b[39m sample_timesteps = torch.linspace(eps, \u001b[32m1.0\u001b[39m, timesteps, device=device)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m imgs_vesde = \u001b[43msample_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_ch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_CH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvesde_sampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(imgs_vesde)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m images using VESDESampler.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m calculate_class_proportions(imgs_vesde, classifier, n_classes=N_CLASSES, device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/src/code/src/utils/other_utils.py:54\u001b[39m, in \u001b[36msample_images\u001b[39m\u001b[34m(model, reverse, timesteps, img_ch, img_size, device, sample_size, plot, save, save_path, *model_args)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(sample_size), desc=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m Sampling images\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     53\u001b[39m     x_t = torch.randn((\u001b[32m1\u001b[39m, img_ch, img_size, img_size), device=device)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     imgs.append(\u001b[43msample_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m plot:\n\u001b[32m     57\u001b[39m     fig, axs = plt.subplots(\u001b[32m1\u001b[39m, sample_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/src/code/src/utils/other_utils.py:26\u001b[39m, in \u001b[36msample_image\u001b[39m\u001b[34m(model, x_t, reverse, timesteps, *model_args)\u001b[39m\n\u001b[32m     24\u001b[39m     t = torch.full((\u001b[32m1\u001b[39m,), t.item(), device=x_t.device)\n\u001b[32m     25\u001b[39m     y = torch.randint(\u001b[32m0\u001b[39m, \u001b[32m10\u001b[39m, (\u001b[32m1\u001b[39m,), device=x_t.device)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     pred_t = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     x_t = reverse(x_t, t, dt, pred_t)\n\u001b[32m     29\u001b[39m t = timesteps[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/src/code/src/models/UNet.py:498\u001b[39m, in \u001b[36mUNet_res.forward\u001b[39m\u001b[34m(self, x, t, y)\u001b[39m\n\u001b[32m    495\u001b[39m h1 = \u001b[38;5;28mself\u001b[39m.conv1(x) + \u001b[38;5;28mself\u001b[39m.dense1(embed)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m## Incorporate information from t\u001b[39;00m\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m## Group normalization\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m h1 = \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh1\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    499\u001b[39m h2 = \u001b[38;5;28mself\u001b[39m.conv2(h1) + \u001b[38;5;28mself\u001b[39m.dense2(embed)\n\u001b[32m    500\u001b[39m h2 = \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28mself\u001b[39m.gnorm2(h2))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/normalization.py:313\u001b[39m, in \u001b[36mGroupNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/functional.py:2965\u001b[39m, in \u001b[36mgroup_norm\u001b[39m\u001b[34m(input, num_groups, weight, bias, eps)\u001b[39m\n\u001b[32m   2958\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2959\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.dim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2960\u001b[39m     )\n\u001b[32m   2961\u001b[39m _verify_batch_size(\n\u001b[32m   2962\u001b[39m     [\u001b[38;5;28minput\u001b[39m.size(\u001b[32m0\u001b[39m) * \u001b[38;5;28minput\u001b[39m.size(\u001b[32m1\u001b[39m) // num_groups, num_groups]\n\u001b[32m   2963\u001b[39m     + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m.size()[\u001b[32m2\u001b[39m:])\n\u001b[32m   2964\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2965\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2966\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2967\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Generate 100 images\n",
    "timesteps = 400\n",
    "eps = 1e-3\n",
    "\n",
    "vesde_sampler = VESDESampler(device=device, img_ch=IMG_CH, img_size=IMG_SIZE)\n",
    "sample_timesteps = torch.linspace(eps, 1.0, timesteps, device=device)\n",
    "\n",
    "imgs_vesde = sample_images(\n",
    "    model=model,\n",
    "    sampler=vesde_sampler,\n",
    "    timesteps=sample_timesteps,\n",
    "    device=device,\n",
    "    sample_size=100,\n",
    "    plot=False,\n",
    "    save=False,\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(imgs_vesde)} images using VESDESampler.\")\n",
    "calculate_class_proportions(imgs_vesde, classifier, n_classes=N_CLASSES, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729fda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t Sampling images: 100%|██████████| 100/100 [03:29<00:00,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 100 images using PDLMCSampler.\n",
      "0: 0.32  1: 0.00  2: 0.00  3: 0.00  4: 0.00  5: 0.00  6: 0.00  7: 0.00  8: 0.00  9: 0.68  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gfuncs = [\n",
    "    partial(classifier_constraint, classifier=classifier, target_class=9, epsilon=0.01)\n",
    "]\n",
    "lmc_steps: int = 1\n",
    "step_size: float = 2\n",
    "step_size_lambda: float = 0.75\n",
    "timesteps = 500\n",
    "sample_timesteps = torch.linspace(eps, 1.0, timesteps, device=device)\n",
    "\n",
    "pdlmc_sampler = PDLMCVESampler(\n",
    "    device=device,\n",
    "    img_ch=IMG_CH,\n",
    "    img_size=IMG_SIZE,\n",
    "    gfuncs=gfuncs,\n",
    "    lmc_steps=lmc_steps,\n",
    "    step_size=step_size,\n",
    "    step_size_lambda=step_size_lambda,\n",
    ")\n",
    "\n",
    "imgs_pdlmc = img = sample_images(\n",
    "    model=model,\n",
    "    sampler=pdlmc_sampler,\n",
    "    timesteps=sample_timesteps,\n",
    "    device=device,\n",
    "    sample_size=100,\n",
    "    plot=False,\n",
    "    save=False,\n",
    ")\n",
    "print(f\"\\nGenerated {len(imgs_pdlmc)} images using PDLMCSampler.\")\n",
    "calculate_class_proportions(imgs_pdlmc, classifier, n_classes=N_CLASSES, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee8331",
   "metadata": {},
   "source": [
    "# Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40de9a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t Sampling images: 100%|██████████| 1/1 [00:14<00:00, 14.24s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAEY1JREFUeJzt3EtvXQfVBuDly7ETx7eYJOSe0FgkQkgdUFUwQwKVIWN+A38Efge/AEYMKBISEqUSEpRWClVubZq0TSzHie3Yji/nm3xaUkv15axFbQLf84zPe/Y+2/ucN3uQd2w4HA4DACJi/N99AgC8OpQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAGly1BeOjY0d5nn8yzrn1/l/e4PBoJw5qnOL6J3f8+fPW8d6lU1PT5cz+/v7h3AmX99xOn/bycmRv+Kpc347OzvlzOzsbDkTEbGxsdHKVU1NTZUzL168OIQz+fqM8rviSQGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABII69lnThxovzmm5ub5czMzEw5E9Ebdbt8+XI58/jx43Jma2urnDl+/Hg5E9G7DhcuXChnHjx4UM4cpc5A2/nz58uZzv36+eeflzMREevr6+VMZ9Stcw91jtO9x49qEO9VH7c7rJFSTwoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAGhsOh8ORXnhI40v/HywtLZUzq6urh3AmX5/l5eVy5tatW61jzc7OljNHNZrWGRPsDM5F9AbkHj582DpW1dmzZ8uZq1evto715z//uZzpnF9nBLR7j8/NzZUznftob2/vpa/xpABAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAGnkldWZmpvzmu7u75cwoK35fpbO4OD8/X87cv3+/nOl48uRJK9f5O3XWFk+dOlXOdJdLt7e3y5nTp0+XM48fPy5nOjqLmBERExMT5Uzn2nUyne/fvXv3ypmj1Fk3Xl9fbx2r87vXWa7e399/6Ws8KQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBpctQXLi4ult/8+PHj5Ux3LGxtba2cee+991rHqrpy5Uo50x3E6wyTDQaDcqYzdnhwcFDORERMT0+XM51xu6mpqXLmzTffLGf++Mc/ljNH6fLly+XMUY7bTU6O/LOVOoNzq6ur5czJkyfLmYje933ELdMyTwoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAGhuOuKo0NjZ22OcSERHj472e6oymHTt2rJzpjPytrKyUMxMTE+VMRMTW1lYrV9UZTXv48GHrWJ0xs4WFhXLml7/8ZTnzne98p5x56623ypmI3qDg6dOny5kHDx6UM53fh5mZmXImImJzc7OcWVpaKmeePn1azuzv75czEb3fos6xXrx48dLXeFIAICkFAJJSACApBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUA0uRhvnln1G1qaqp1rEuXLpUzH374YTnTGSUbZYTqyzoDWUepM5o2Pz/fOlZnEG9nZ6eceeONN8qZ733ve+XMYDAoZyIinj9/Xs50/k4dI+5qfsGFCxdax1pbWytnHj16VM6cPHmynLl+/Xo5ExHxzjvvlDPd8dCXvu+hvCsA/5GUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAGnkQbzjx4+X37wzHtcdguuM23V0htaWl5fLmc6IXkTExx9/XM50hsk659e5hyJ6n6lzH83NzZUzR3mP7+/vlzPnz58vZ27dulXOdD5TZ9guojdu17kOnfPrDNtF9L4bu7u7rWO9jCcFAJJSACApBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUAklIAICkFANLYcDgcjvLCzopfZ9VxbGysnInor4q+qmZnZ1u5znXoZE6dOlXOrKyslDMREVeuXClnVldXy5nf/OY35cwPf/jDcuZb3/pWORMR8dFHH5UzI369v+DatWvlTGc9eGNjo5yJiDhz5kw501l+7Szgjo/3/p3d+d3r/L6Ocj94UgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUAklIAICkFAJJSACApBQDS5Kgv7Ay0bW9vlzN7e3vlzKvu8uXL5cza2lrrWDMzM+VMZxBvMBiUM8vLy+VMRG/MbHp6upzpXLtPPvmknOkMrUX0rnnnb3v//v1yZmpqqpzp3g9//etfW7mqxcXFcqYzHBrR+753RghH4UkBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASCMP4nVGvIbDYTmzv79fzkT0xsyeP39ezly5cqWc+eijj8qZzvhZRMTu7m4rV/Xpp5+WM8eOHTuEM/lqnXtvfLz+b6S//OUv5czKyko5E9Ebt+u4evVqOfPZZ5+VM91hux//+MflzO9+97typjNSt7GxUc5ERMzNzZUzW1tbrWO9jCcFAJJSACApBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUAklIAII08iLe6unqY55Fu3LjRyt28ebOcWV5eLmdu3bpVziwtLR1JJqI3iNcZ7Pv2t79dzty9e7eciYiYnp5u5aquX79ezvzqV78qZ7qjj0flww8//Hefwv/p73//ezlz6dKlcqYz+ri3t1fORPTGDicnR/75LvGkAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEA6nJm9/3XmzJlyprN2GhGxuLhYznQWTzsmJibKme65LSwslDM//elPy5lf//rX5cxRGgwG5UxnCfj3v/99ObOzs1PO/Deamppq5WZnZ8uZ27dvt45V1VkPjoi4f/9+OdNdZH0ZTwoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAOtRBvJmZmcN8+y/Y3d0tZzojemtra+XM9evXy5nHjx+XMxERp0+fLmde9XG7jp///OflzLlz58qZra2tcmZubq6ciYhYX18vZ65du1bO3Llzp5wZH6//+/LFixflTERv3G5sbKycGQ6H5cyTJ0/KmYiIg4ODVu4weFIAICkFAJJSACApBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUA0thwxNWnzqBUx+XLl1u5jz/++Gs+k6/2ox/9qJx5++23y5kTJ06UMxERm5ubrdxRmJ+fb+U6o3N/+MMfypnp6ely5vvf/3450xmpi4i4efNmK1d1/vz5cub48ePlTGfY7r9V5/ptb2+XM6MM73lSACApBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUAklIAICkFANLkv/sEvuzx48et3OLiYjmztrZWznTG7ZaWlsqZ1dXVcqZ7rBcvXpQzGxsb5Uzn3CIiHjx4UM6MuPP4BX/729/KmWPHjpUz3WG7CxculDOda/fw4cNyZmJiopzp3g+jjLp9Wee7PhgMypnuZ+qcX+eaj8KTAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJAOdRBvbGysnNna2moda3t7u5U7Ct1xu47XXnutnOkMtL3++uvlzAcffFDOREScP3++las6ffp0ObO+vn4IZ/LVOuN2b775Zjnz7rvvljOzs7PlTGeIMaI3xtixt7dXzpw4caJ1rM8//7yc6fy+jsKTAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBpbDgcDkd54dLSUvnNFxYWypnOMmFExOLiYjnz/vvvlzNTU1PlTHcNsqNzzU+ePFnO3Lt3r5y5ePFiORMR8fz583Lm7t275cw//vGPcuatt94qZ0b8yv2Tp0+fljNzc3PlTGf5dX5+vpw5c+ZMORMRcevWrXKm872dnp4uZ7qruYPBoJw5ODgoZ0b5ffWkAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKAKTJUV/45MmT8pt3BqW+8Y1vlDMREVtbW61c1VGN23UGxiJ616E7Qli1trbWynXOb3y8/u+dzujj2NhYOXP27NlyJiJidna2nOkMrXVG3Z49e3Ykx+nqfG87I5vdz9QZ7Nvc3Gwd62U8KQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBp5EG8js4o2QcffHAIZ/LVlpeXj+Q4nZG6Bw8eHMKZfLWjGvnb3d1t5TrDisPhsJzpDPZ985vfLGdu3rxZznR1hhWPHz9eziwsLJQzn332WTkTEXHhwoVy5uDgoJzpfKZHjx6VMxG978bMzEzrWC/jSQGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABIhzqINzU1dZhv/y+7detWOdMZ+bt69Wo5MxgMypmI/uhc1cTERDmzs7PTOlYn9/Tp03Km85kePnxYznSdO3eunNne3i5nnj17Vs6cOHGinOkM70X0xiI7v0WffvppOfPd7363nImIeP/998uZwxqy9KQAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoApJEH8WZmZspv3hlsWl5eLmciIu7cuVPOdEayFhYWypnOtesO2127dq2c6QzOffLJJ+VM1+zsbDmzuLhYzvzpT38qZzr3eHcI7qgG2iYn6zuZnUG8lZWVcqbrxo0b5cx7771XznSG7SIipqeny5m9vb3WsV7GkwIASSkAkJQCAEkpAJCUAgBJKQCQlAIASSkAkJQCAEkpAJCUAgBJKQCQlAIAaWw4HA5HeWFnqXJzc7Oc6aw6RkRcvHixnPntb39bzszPz5czz549K2euXr1azkREPHz4sJw5e/ZsObO/v1/OPHjwoJyJ6K3ZvvPOO+VM52/7k5/8pJy5fft2ORMRcf78+XKmcz90dJZfuyuf3QXhqs7S7traWutYnZXUznUY5XvrSQGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYCkFABIIw/iLS0tld98YWGhnLl371450zU3N1fOnDt3rpy5e/duOdMZyIroDendvHmznJmcnCxnOvdQRG/U7Wc/+1k584tf/KKc+cEPflDOHNVIXURvyHJra6uc6Qwkdk1MTJQzr7/+ejnT+S168eJFORMRsbGxUc4MBoNyZpTz86QAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoApJEH8cbGxspvPjU1Vc7Mz8+XMxERKysr5cypU6fKmddee62ceffdd8uZ8fFeXx8cHLRyVRcvXixnOn+jiN7I3507d8qZ3d3dcmbEr88X3Lhxo5yJ6A0XHpU33nijnLl9+3brWE+ePGnljkJnpC6id+8ZxAPg0CkFAJJSACApBQCSUgAgKQUAklIAICkFAJJSACApBQCSUgAgKQUA0qEO4nV0RvQiIs6ePVvOrK2tlTNbW1vlTGfsqjPWF9E7v0uXLpUzr/I426vuzJkzrdyjR4++5jP5z9S5fqMMwX1Z5/eha25urpzZ2NgoZ0YZzPSkAEBSCgAkpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAEA61JXUzuJpZ82wazAYlDPHjh0rZ9bX18uZrtOnT5czjx8/PoQz+Wed693NPX/+vHWsqs6a7crKyiGcyVebmZk5kuN0rvfi4mLrWJ110L29vdaxqrr3+Ph4/d/nOzs75cwoP/eeFABISgGApBQASEoBgKQUAEhKAYCkFABISgGApBQASEoBgKQUAEhKAYB0qIN4r7rOYF/nOnSGqyYmJsqZiN757e/vH8lxuuNsnbG1EW/rL+iMmXWG1jr3XfdYnaG1o9Idv+x8ps4173wvusN7ne/G5uZmOWMQD4ASpQBAUgoAJKUAQFIKACSlAEBSCgAkpQBAUgoAJKUAQFIKACSlAECaHPWFnYExAP6zeFIAICkFAJJSACApBQCSUgAgKQUAklIAICkFAJJSACD9D1ehNDlJh53CAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 1 images using PDLMCSampler.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.05441856384277344"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gfuncs = [\n",
    "    partial(brightness_constraint, target=0.1, tol=0.005)\n",
    "]\n",
    "\n",
    "lmc_steps: int = 1\n",
    "update_steps: int = 10\n",
    "step_size: float = 0.1\n",
    "step_size_lambda: float = 0.01\n",
    "timesteps = 500\n",
    "\n",
    "sample_timesteps = torch.linspace(eps, 1.0, timesteps, device=device)\n",
    "\n",
    "pdlmc_sampler = PDLMCVESampler(\n",
    "    device=device,\n",
    "    img_ch=IMG_CH,\n",
    "    img_size=IMG_SIZE,\n",
    "    gfuncs=gfuncs,\n",
    "    lmc_steps=lmc_steps,\n",
    "    step_size=step_size,\n",
    "    step_size_lambda=step_size_lambda,\n",
    ")\n",
    "\n",
    "imgs_pdlmc = img = sample_images(\n",
    "    model=model,\n",
    "    sampler=pdlmc_sampler,\n",
    "    timesteps=sample_timesteps,\n",
    "    device=device,\n",
    "    sample_size=1,\n",
    "    plot=True,\n",
    "    save=False,\n",
    "    update_steps=update_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(imgs_pdlmc)} images using PDLMCSampler.\")\n",
    "calculate_mean_brightness(imgs_pdlmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3561f1c3",
   "metadata": {},
   "source": [
    "# Centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f330eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t Sampling images: 100%|██████████| 1/1 [00:06<00:00,  6.13s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 400x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 1 images using PDLMCSampler.\n"
     ]
    }
   ],
   "source": [
    "gfuncs = [\n",
    "    partial(center_constraint, center=(6, 20))\n",
    "]\n",
    "\n",
    "lmc_steps: int = 1\n",
    "update_steps: int = 10\n",
    "step_size: float = 0.1\n",
    "step_size_lambda: float = 0.01\n",
    "timesteps = 500\n",
    "\n",
    "sample_timesteps = torch.linspace(eps, 1.0, timesteps, device=device)\n",
    "\n",
    "pdlmc_sampler = PDLMCVESampler(\n",
    "    device=device,\n",
    "    img_ch=IMG_CH,\n",
    "    img_size=IMG_SIZE,\n",
    "    gfuncs=gfuncs,\n",
    "    lmc_steps=lmc_steps,\n",
    "    step_size=step_size,\n",
    "    step_size_lambda=step_size_lambda,\n",
    ")\n",
    "\n",
    "imgs_pdlmc = img = sample_images(\n",
    "    model=model,\n",
    "    sampler=pdlmc_sampler,\n",
    "    timesteps=sample_timesteps,\n",
    "    device=device,\n",
    "    sample_size=1,\n",
    "    plot=True,\n",
    "    save=False,\n",
    "    update_steps=update_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(imgs_pdlmc)} images using PDLMCSampler.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
