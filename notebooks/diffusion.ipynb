{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d13d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/usr/src/code/src')\n",
    "os.chdir('/usr/src/code/')\n",
    "\n",
    "import torch\n",
    "\n",
    "from src.models.UNet import UNet, UNet2DWrapper, UNet_res, UNet_Tranformer\n",
    "from src.utils.other_utils import *\n",
    "from src.pdlmc.constraints import classifier_constraint, brightness_constraint, center_constraint\n",
    "from src.utils.samplers import *\n",
    "from src.utils.trainers import *\n",
    "from src.utils.schedulers import *\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff5fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28\n",
    "IMG_CH = 1\n",
    "BATCH_SIZE = 128\n",
    "N_CLASSES = 10\n",
    "timesteps = 250\n",
    "sigma = 25.0\n",
    "eps = 1e-3\n",
    "\n",
    "model = UNet_Tranformer(partial(marginal_prob_std, sigma=sigma, device=device))\n",
    "model.load_state_dict(torch.load(\"unet_transformer.pth\"))\n",
    "model = model.to(device)\n",
    "classifier = torch.load(\"mnist_classifier.pkl\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00075ba0",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf218257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t Sampling images: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timesteps = 400\n",
    "eps = 1e-3\n",
    "\n",
    "vesde_sampler = VESDESampler(device=device, img_ch=IMG_CH, img_size=IMG_SIZE)\n",
    "sample_timesteps = torch.linspace(eps, 1.0, timesteps, device=device)\n",
    "img = sample_images(\n",
    "    model=model,\n",
    "    sampler=vesde_sampler,\n",
    "    timesteps=sample_timesteps,\n",
    "    device=device,\n",
    "    sample_size=1,\n",
    "    plot=True,\n",
    "    save=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74329636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t Sampling images:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     11\u001b[39m pdlmc_sampler = PDLMCVESampler(\n\u001b[32m     12\u001b[39m     device=device,\n\u001b[32m     13\u001b[39m     img_ch=IMG_CH,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     step_size_lambda=step_size_lambda,\n\u001b[32m     19\u001b[39m )\n\u001b[32m     21\u001b[39m sample_timesteps = torch.linspace(eps, \u001b[32m1.0\u001b[39m, timesteps, device=device)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m img = \u001b[43msample_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdlmc_sampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupdate_steps\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/src/code/src/utils/other_utils.py:56\u001b[39m, in \u001b[36msample_images\u001b[39m\u001b[34m(model, sampler, timesteps, device, sample_size, plot, save, save_path, **kwargs)\u001b[39m\n\u001b[32m     52\u001b[39m     x_t = torch.randn(\n\u001b[32m     53\u001b[39m         (\u001b[32m1\u001b[39m, sampler.img_ch, sampler.img_size, sampler.img_size), device=device\n\u001b[32m     54\u001b[39m     )\n\u001b[32m     55\u001b[39m     y = torch.randint(\u001b[32m0\u001b[39m, \u001b[32m10\u001b[39m, (\u001b[32m1\u001b[39m,), device=x_t.device)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     imgs.append(\u001b[43msampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m plot:\n\u001b[32m     59\u001b[39m     plt.figure(figsize=(\u001b[32m4\u001b[39m * sample_size, \u001b[32m4\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/src/code/src/utils/samplers.py:224\u001b[39m, in \u001b[36mPDLMCVESampler.sample\u001b[39m\u001b[34m(self, x_t, model, timesteps, y, **kwargs)\u001b[39m\n\u001b[32m    222\u001b[39m dt = timesteps[\u001b[32m1\u001b[39m] - timesteps[\u001b[32m0\u001b[39m]\n\u001b[32m    223\u001b[39m sigma_T = \u001b[38;5;28mself\u001b[39m.marginal_prob_std(timesteps[-\u001b[32m1\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m x_t = x_t * \u001b[43msigma_T\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\n\u001b[32m    225\u001b[39m num_steps = \u001b[38;5;28mlen\u001b[39m(timesteps)\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(timesteps)):\n",
      "\u001b[31mIndexError\u001b[39m: too many indices for tensor of dimension 0"
     ]
    }
   ],
   "source": [
    "gfuncs = [\n",
    "    partial(classifier_constraint, classifier=classifier, target_class=9, epsilon=0.01)\n",
    "]\n",
    "\n",
    "lmc_steps: int = 1\n",
    "update_steps: int = 3\n",
    "step_size: float = 2\n",
    "step_size_lambda: float = 0.2\n",
    "timesteps = 400\n",
    "\n",
    "pdlmc_sampler = PDLMCVESampler(\n",
    "    device=device,\n",
    "    img_ch=IMG_CH,\n",
    "    img_size=IMG_SIZE,\n",
    "    gfuncs=gfuncs,\n",
    "    lmc_steps=lmc_steps,\n",
    "    step_size=step_size,\n",
    "    step_size_lambda=step_size_lambda,\n",
    ")\n",
    "\n",
    "sample_timesteps = torch.linspace(eps, 1.0, timesteps, device=device)\n",
    "img = sample_images(\n",
    "    model=model,\n",
    "    sampler= pdlmc_sampler,\n",
    "    timesteps=sample_timesteps,\n",
    "    device=device,\n",
    "    sample_size=1,\n",
    "    plot=True,\n",
    "    save=False,\n",
    "    update_steps = update_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719bdbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t Sampling images:   1%|          | 1/100 [00:01<02:25,  1.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m vesde_sampler = VESDESampler(device=device, img_ch=IMG_CH, img_size=IMG_SIZE)\n\u001b[32m      6\u001b[39m sample_timesteps = torch.linspace(eps, \u001b[32m1.0\u001b[39m, timesteps, device=device)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m imgs_vesde = \u001b[43msample_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_ch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_CH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvesde_sampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(imgs_vesde)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m images using VESDESampler.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m calculate_class_proportions(imgs_vesde, classifier, n_classes=N_CLASSES, device=device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/src/code/src/utils/other_utils.py:54\u001b[39m, in \u001b[36msample_images\u001b[39m\u001b[34m(model, reverse, timesteps, img_ch, img_size, device, sample_size, plot, save, save_path, *model_args)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(sample_size), desc=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m Sampling images\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     53\u001b[39m     x_t = torch.randn((\u001b[32m1\u001b[39m, img_ch, img_size, img_size), device=device)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     imgs.append(\u001b[43msample_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m plot:\n\u001b[32m     57\u001b[39m     fig, axs = plt.subplots(\u001b[32m1\u001b[39m, sample_size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/src/code/src/utils/other_utils.py:26\u001b[39m, in \u001b[36msample_image\u001b[39m\u001b[34m(model, x_t, reverse, timesteps, *model_args)\u001b[39m\n\u001b[32m     24\u001b[39m     t = torch.full((\u001b[32m1\u001b[39m,), t.item(), device=x_t.device)\n\u001b[32m     25\u001b[39m     y = torch.randint(\u001b[32m0\u001b[39m, \u001b[32m10\u001b[39m, (\u001b[32m1\u001b[39m,), device=x_t.device)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     pred_t = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     x_t = reverse(x_t, t, dt, pred_t)\n\u001b[32m     29\u001b[39m t = timesteps[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/src/code/src/models/UNet.py:498\u001b[39m, in \u001b[36mUNet_res.forward\u001b[39m\u001b[34m(self, x, t, y)\u001b[39m\n\u001b[32m    495\u001b[39m h1 = \u001b[38;5;28mself\u001b[39m.conv1(x) + \u001b[38;5;28mself\u001b[39m.dense1(embed)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m## Incorporate information from t\u001b[39;00m\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m## Group normalization\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m498\u001b[39m h1 = \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh1\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    499\u001b[39m h2 = \u001b[38;5;28mself\u001b[39m.conv2(h1) + \u001b[38;5;28mself\u001b[39m.dense2(embed)\n\u001b[32m    500\u001b[39m h2 = \u001b[38;5;28mself\u001b[39m.act(\u001b[38;5;28mself\u001b[39m.gnorm2(h2))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/normalization.py:313\u001b[39m, in \u001b[36mGroupNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/torch/nn/functional.py:2965\u001b[39m, in \u001b[36mgroup_norm\u001b[39m\u001b[34m(input, num_groups, weight, bias, eps)\u001b[39m\n\u001b[32m   2958\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2959\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.dim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2960\u001b[39m     )\n\u001b[32m   2961\u001b[39m _verify_batch_size(\n\u001b[32m   2962\u001b[39m     [\u001b[38;5;28minput\u001b[39m.size(\u001b[32m0\u001b[39m) * \u001b[38;5;28minput\u001b[39m.size(\u001b[32m1\u001b[39m) // num_groups, num_groups]\n\u001b[32m   2963\u001b[39m     + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m.size()[\u001b[32m2\u001b[39m:])\n\u001b[32m   2964\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2965\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2966\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2967\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Generate 100 images\n",
    "timesteps = 400\n",
    "eps = 1e-3\n",
    "\n",
    "vesde_sampler = VESDESampler(device=device, img_ch=IMG_CH, img_size=IMG_SIZE)\n",
    "sample_timesteps = torch.linspace(eps, 1.0, timesteps, device=device)\n",
    "\n",
    "imgs_vesde = sample_images(\n",
    "    model=model,\n",
    "    sampler=vesde_sampler,\n",
    "    timesteps=sample_timesteps,\n",
    "    device=device,\n",
    "    sample_size=100,\n",
    "    plot=False,\n",
    "    save=False,\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(imgs_vesde)} images using VESDESampler.\")\n",
    "calculate_class_proportions(imgs_vesde, classifier, n_classes=N_CLASSES, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729fda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t Sampling images: 100%|██████████| 100/100 [03:29<00:00,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 100 images using PDLMCSampler.\n",
      "0: 0.32  1: 0.00  2: 0.00  3: 0.00  4: 0.00  5: 0.00  6: 0.00  7: 0.00  8: 0.00  9: 0.68  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gfuncs = [\n",
    "    partial(classifier_constraint, classifier=classifier, target_class=9, epsilon=0.01)\n",
    "]\n",
    "lmc_steps: int = 1\n",
    "step_size: float = 2\n",
    "step_size_lambda: float = 0.75\n",
    "timesteps = 500\n",
    "sample_timesteps = torch.linspace(eps, 1.0, timesteps, device=device)\n",
    "\n",
    "pdlmc_sampler = PDLMCVESampler(\n",
    "    device=device,\n",
    "    img_ch=IMG_CH,\n",
    "    img_size=IMG_SIZE,\n",
    "    gfuncs=gfuncs,\n",
    "    lmc_steps=lmc_steps,\n",
    "    step_size=step_size,\n",
    "    step_size_lambda=step_size_lambda,\n",
    ")\n",
    "\n",
    "imgs_pdlmc = img = sample_images(\n",
    "    model=model,\n",
    "    sampler=pdlmc_sampler,\n",
    "    timesteps=sample_timesteps,\n",
    "    device=device,\n",
    "    sample_size=100,\n",
    "    plot=False,\n",
    "    save=False,\n",
    ")\n",
    "print(f\"\\nGenerated {len(imgs_pdlmc)} images using PDLMCSampler.\")\n",
    "calculate_class_proportions(imgs_pdlmc, classifier, n_classes=N_CLASSES, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee8331",
   "metadata": {},
   "source": [
    "# Brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40de9a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t Sampling images: 100%|██████████| 1/1 [00:11<00:00, 11.09s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 1 images using PDLMCSampler.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07076753675937653"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gfuncs = [\n",
    "    partial(brightness_constraint, target=0.3, tol=0.005)\n",
    "]\n",
    "\n",
    "lmc_steps: int = 1\n",
    "update_steps: int = 5\n",
    "step_size: float = 5\n",
    "step_size_lambda: float = 5\n",
    "timesteps = 500\n",
    "\n",
    "sample_timesteps = torch.linspace(eps, 1.0, timesteps, device=device)\n",
    "\n",
    "pdlmc_sampler = PDLMCVESampler(\n",
    "    device=device,\n",
    "    img_ch=IMG_CH,\n",
    "    img_size=IMG_SIZE,\n",
    "    gfuncs=gfuncs,\n",
    "    lmc_steps=lmc_steps,\n",
    "    step_size=step_size,\n",
    "    step_size_lambda=step_size_lambda,\n",
    ")\n",
    "\n",
    "imgs_pdlmc = img = sample_images(\n",
    "    model=model,\n",
    "    sampler=pdlmc_sampler,\n",
    "    timesteps=sample_timesteps,\n",
    "    device=device,\n",
    "    sample_size=1,\n",
    "    plot=True,\n",
    "    save=False,\n",
    "    update_steps=update_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(imgs_pdlmc)} images using PDLMCSampler.\")\n",
    "calculate_mean_brightness(imgs_pdlmc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3561f1c3",
   "metadata": {},
   "source": [
    "# Centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08f330eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t Sampling images: 100%|██████████| 1/1 [00:08<00:00,  8.32s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 1 images using PDLMCSampler.\n"
     ]
    }
   ],
   "source": [
    "gfuncs = [\n",
    "    partial(center_constraint, center=(6, 20))\n",
    "]\n",
    "\n",
    "lmc_steps: int = 1\n",
    "update_steps: int = 3\n",
    "step_size: float = 1\n",
    "step_size_lambda: float = 0.5\n",
    "timesteps = 500\n",
    "\n",
    "sample_timesteps = torch.linspace(eps, 1.0, timesteps, device=device)\n",
    "\n",
    "pdlmc_sampler = PDLMCVESampler(\n",
    "    device=device,\n",
    "    img_ch=IMG_CH,\n",
    "    img_size=IMG_SIZE,\n",
    "    gfuncs=gfuncs,\n",
    "    lmc_steps=lmc_steps,\n",
    "    step_size=step_size,\n",
    "    step_size_lambda=step_size_lambda,\n",
    ")\n",
    "\n",
    "imgs_pdlmc = img = sample_images(\n",
    "    model=model,\n",
    "    sampler=pdlmc_sampler,\n",
    "    timesteps=sample_timesteps,\n",
    "    device=device,\n",
    "    sample_size=1,\n",
    "    plot=True,\n",
    "    save=False,\n",
    "    update_steps=update_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(imgs_pdlmc)} images using PDLMCSampler.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
